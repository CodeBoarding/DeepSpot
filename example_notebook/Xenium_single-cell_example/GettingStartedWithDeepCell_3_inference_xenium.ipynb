{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da1eea45-93e6-43ca-9147-ac156ce13688",
   "metadata": {},
   "source": [
    "## DeepCell inference from H&E images\n",
    "\n",
    "Here, we provide an example of how to use the pretrained weights and perform inference using DeepCell to predict spatial transcriptomics from H&E images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1374995c-6ade-43b9-8c16-f78d6fbca88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../../')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd210287-0500-4447-9efb-d2c0be5e74be",
   "metadata": {},
   "source": [
    "Export packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7379e96d-02c3-4dc6-aaa1-5fe23eb25c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepspot.utils.utils_image import predict_cell_spatial_transcriptomics_from_image_path\n",
    "from deepspot.utils.utils_image import get_morphology_model_and_preprocess\n",
    "from deepspot.utils.utils_image import crop_tile\n",
    "\n",
    "from deepspot.cell import DeepCell\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "from openslide import open_slide\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import scanpy as sc\n",
    "import squidpy as sq\n",
    "import anndata as ad\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyvips\n",
    "import torch\n",
    "import math\n",
    "import yaml\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e15f58-bce4-45b8-a9a0-ae2b9e8feae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb569dc1-3d7f-4af7-9210-7ef5580be625",
   "metadata": {},
   "source": [
    "Here, we specify the input parameters. This information should be selected carefully, as it is based on the single-cell spatial transcriptomics training dataset. We continue with values based on our toy example from the Lung Xenium dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723fbf76-6a3e-4037-8c35-320025c3470e",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_folder = \"example_data\"\n",
    "image_feature_model = 'inception' \n",
    "cell_diameter = 20 # cell diameter\n",
    "n_neighbors = 45 # the n_neighbors used to compute the neighbors around \n",
    "downsample_factor = 10 # downsampling the image used for visualisation in squidpy\n",
    "model_weights = 'pretrained_model_weights/example_model/weights_Xenium.pkl'\n",
    "model_hparam = 'pretrained_model_weights/example_model/hparam_Xenium.yaml'\n",
    "gene_path = f\"{out_folder}/data/info_highly_variable_genes_Xenium.csv\"\n",
    "sample = 'NCBI858'\n",
    "image_path = f'example_data/data/image/{sample}.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4d601f-2f1b-4b44-8e9f-0a42c9b03f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "genes = pd.read_csv(gene_path)\n",
    "selected_genes_bool = genes.isPredicted.values\n",
    "genes_to_predict = genes[selected_genes_bool]\n",
    "genes_to_predict.sort_values(\"highly_variable_rank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26facee3-018a-4667-9361-c1bb64e89369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image\n",
    "image = mpimg.imread(image_path)\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(image)\n",
    "plt.axis('off')  # Turn off axis labels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d22b812-4c4b-433d-9c20-289fc9bda853",
   "metadata": {},
   "source": [
    "Normally, one should run a cell segmentation pipeline to determine the precise locations of the cells. However, for simplicity in this notebook, we skip this step and assume that the spatial cell coordinates are already available by reusing the real ones from the toy example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac0c831-4bcf-4388-ad06-952596a04b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "coord = sc.read_h5ad(f\"example_data/data/h5ad/{sample}.h5ad\").obs[[\"x_pixel\", \"y_pixel\"]].copy()\n",
    "coord"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60a45ee-55ef-446a-9274-38e6ba0d17fa",
   "metadata": {},
   "source": [
    "We create the anndata object, empty for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64eb2c7-4f8a-4d71-9b34-b88b71f052a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = np.empty((len(coord), selected_genes_bool.sum())) # empty count matrix \n",
    "adata = ad.AnnData(counts).copy()\n",
    "adata.obs.index = coord.index\n",
    "adata.var.index = genes[selected_genes_bool].gene_name.values\n",
    "adata.obs = adata.obs.merge(coord, left_index=True, right_index=True)\n",
    "adata.obs['sampleID'] = sample\n",
    "adata.obs['barcode'] = adata.obs.index\n",
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c1561e-d0d4-4b19-8b47-9f0e75d25fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CREATE IMAGE\n",
    "img = open_slide(image_path)\n",
    "n_level = len(img.level_dimensions) - 1 # 0 based\n",
    "\n",
    "\n",
    "large_w, large_h = img.dimensions\n",
    "new_w = math.floor(large_w / downsample_factor)\n",
    "new_h = math.floor(large_h / downsample_factor)\n",
    "print(large_w, large_h, new_w, new_h)\n",
    "whole_slide_image = img.read_region((0, 0), n_level, img.level_dimensions[-1])\n",
    "whole_slide_image = whole_slide_image.convert(\"RGB\")\n",
    "img_downsample = whole_slide_image.resize((new_w, new_h), PIL.Image.BILINEAR)\n",
    "\n",
    "\n",
    "adata.obsm['spatial'] = adata.obs[[\"y_pixel\", \"x_pixel\"]].values\n",
    "# adjust coordinates to new image dimensions\n",
    "adata.obsm['spatial'] = adata.obsm['spatial'] / downsample_factor\n",
    "# create 'spatial' entries\n",
    "adata.uns['spatial'] = dict()\n",
    "adata.uns['spatial']['library_id'] = dict()\n",
    "adata.uns['spatial']['library_id']['images'] = dict()\n",
    "adata.uns['spatial']['library_id']['images']['hires'] = np.array(img_downsample)\n",
    "img_downsample.width, img_downsample.height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbedf95-7f96-4638-8364-05a7a06d667e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the YAML file into a regular Python dictionary\n",
    "with open(model_hparam, 'r') as yaml_file:\n",
    "    model_hparam = yaml.safe_load(yaml_file)\n",
    "model_hparam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc94bb5c-243a-4cc1-9ebb-4c774c7ed49a",
   "metadata": {},
   "source": [
    "Initialize DeepCell and the pretrained pathology foundation model. This time, we compute the tile representation on the fly, which may take more time. The current implementation preprocesses a single cell per batch, but extending it to multiple cells might offer additional speed improvements. Contributions are welcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b1dc5c-556c-4ea6-8b53-96ae555f4246",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_expression = torch.load(model_weights, map_location=device)\n",
    "model_expression.to(device)\n",
    "model_expression.eval()\n",
    "\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ade376-cc19-4c44-b283-46173ca253cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "morphology_model, preprocess, feature_dim = get_morphology_model_and_preprocess(model_name=image_feature_model, device=device)\n",
    "morphology_model.to(device)\n",
    "morphology_model.eval()\n",
    "\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907ee30d-0c27-444f-94e7-d536fb96c69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = predict_cell_spatial_transcriptomics_from_image_path(image_path, \n",
    "                                                        adata,\n",
    "                                                        cell_diameter,\n",
    "                                                        n_neighbors,\n",
    "                                                        preprocess, \n",
    "                                                        morphology_model, \n",
    "                                                        model_expression, \n",
    "                                                        device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cbfcb2-1bce-44b1-ac56-d68bfec9150c",
   "metadata": {},
   "source": [
    "##### Remember from the training notebook...\n",
    "The `scaler` is important to be the same as the one used during training, so that the predictions of DeepCell can be rescaled back to their original ranges using the `inverse_transform` function. \n",
    "\n",
    "##### IMPORTANT: Remember to manually rescale the values, as this is not done automatically.\n",
    "```\n",
    "expression_norm = model(X)\n",
    "expression_norm should be np.array\n",
    "expression = model.inverse_transform(expression_norm)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c24b42d-a110-4f83-ad1d-04631ffd4996",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = model_expression.inverse_transform(counts)\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3802bd20-43f5-44ca-b867-42cc38f3c16f",
   "metadata": {},
   "source": [
    "You are free to explore other types of transformations that may enhance spatial transcriptomics predictions. The following are just a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61d2d02-169a-4192-8158-ecd03f4d0bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts[counts < 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea79b9e7-6fd6-489c-a322-f61d51e958e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_predicted = ad.AnnData(counts, \n",
    "                             var=adata.var,\n",
    "                             obs=adata.obs, \n",
    "                             uns=adata.uns, \n",
    "                             obsm=adata.obsm).copy()\n",
    "adata_predicted.obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36535275-d43b-4f8a-82b0-ea2030af0369",
   "metadata": {},
   "outputs": [],
   "source": [
    "sq.pl.spatial_scatter(adata_predicted, \n",
    "                      color=['CCL18', 'POSTN', \n",
    "                             'PGC', 'LAMP3'], \n",
    "                      wspace=0,\n",
    "                      ncols=2,\n",
    "                      size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f137d7fb-dc44-4a97-80ae-9cf6ab6a555a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nonchev",
   "language": "python",
   "name": "nonchev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
