{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a95ec865-0b03-4e45-8f30-d1d219710b4f",
   "metadata": {},
   "source": [
    "## Spatial transcriptomics preprocessing - single-cell Xenium data\n",
    "\n",
    "In these GettingStarted notebooks, we will guide you through the process of preprocessing, training, and performing inference with DeepCell on your single-cell spatial transcriptomics data (Xenium). First, we will begin with data preprocessing. In the training notebook, we will demonstrate how to train DeepCell, adjust hyperparameters, and export the model weights. Finally, in the inference notebook, we will show you how to load the model weights and perform single-cell spatial transcriptomics prediction using H&E images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27e85b9-37aa-4070-8537-a0c8d4f32f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../../')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7f9f4c-7bf3-44a8-8f4b-9c5f41a91546",
   "metadata": {},
   "source": [
    "Export packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212bc105-af0c-4214-bdff-944d46fceaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepspot.utils.utils_image import get_morphology_model_and_preprocess\n",
    "from deepspot.utils.utils_image import crop_tile\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyvips\n",
    "import torch\n",
    "import glob\n",
    "import yaml\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef214d31-64ff-4f71-aee9-edf5e569beb9",
   "metadata": {},
   "source": [
    "Specify the input files. For this example, we have selected one Xenium sample from the 'Image-based spatial transcriptomics identifies molecular niche dysregulation associated with distal lung remodeling in pulmonary fibrosis' dataset [1], which was downloaded using the HEST1K pipeline [2]. You can modify this pipeline to process multiple samples. The goal is to help you understand the underlying logic.\n",
    "\n",
    "[1] Vannan, A., Lyu, R., Williams, A.L., Negretti, N.M., Mee, E.D., Hirsh, J., Hirsh, S., Hadad, N., Nichols, D.S., Calvi, C.L. and Taylor, C.J., 2025. Spatial transcriptomics identifies molecular niche dysregulation associated with distal lung remodeling in pulmonary fibrosis. Nature Genetics, pp.1-12.\n",
    "\n",
    "[2] Jaume, G., Doucet, P., Song, A. H., Lu, M. Y., Almagro-PÃ©rez, C., Wagner, S. J., ... & Mahmood, F. (2024). Hest-1k: A dataset for spatial transcriptomics and histology image analysis. arXiv preprint arXiv:2406.16192."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5102f8f2-2be4-43b6-892a-7db1cc97c312",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d11d2e9-2ea2-4b22-8ef3-d47f8f854963",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_feature_model = \"inception\" # foundation model \n",
    "sample = \"NCBI858\" # Lung cancer dataset sample\n",
    "out_folder = \"example_data\"\n",
    "adata_in = f\"example_data/data/h5ad/{sample}.h5ad\"\n",
    "json_path = f\"example_data/data/meta/{sample}.json\"\n",
    "img_path = f\"example_data/data/image/{sample}.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087db32a-d4c7-4cc3-af40-5262133f5b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_diameter = 20 # cell diameter\n",
    "cell_diameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66cb206-2051-4430-93b0-6201780ea2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folder to save tile embeddings\n",
    "folder_to_create = f\"{out_folder}/data/image_features/{image_feature_model}_{cell_diameter}/{sample}\"\n",
    "\n",
    "Path(folder_to_create).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "Path(f\"{out_folder}/data/inputX\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e229c5-149b-40ee-b191-ed72dc6dbf76",
   "metadata": {},
   "source": [
    "We start by loading the spatial transcriptomics data and selecting the most variable genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24572520-d00b-4e81-b692-8c743d7d1b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.read_h5ad(adata_in)\n",
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121cc4ce-a2a7-4406-9885-e2692b09c4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.highly_variable_genes(adata, flavor='seurat_v3_paper', \n",
    "                            n_top_genes=500)\n",
    "adata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae3bfad-5c8a-414b-8eb8-486337783ad9",
   "metadata": {},
   "source": [
    "We select the most highly variable genes in the isPredicted variable and save this table. Later, it is useful to understand which genes are the most variable (e.g., based on their rank)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97177a69-27a7-4af3-b78d-278535dc590d",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.var[\"isPredicted\"] = adata.var.highly_variable.values\n",
    "adata.var[\"gene_name\"] = adata.var.index.values\n",
    "adata.var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef206c11-305f-47ac-84be-68400e4a3aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.var.to_csv(f\"{out_folder}/data/info_highly_variable_genes_Xenium.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec431b0-b71b-4cad-afa5-a2e911282884",
   "metadata": {},
   "source": [
    "Here, we save only the counts and store them as a pickle file to make them quickly accessible during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6cc1c0-34b9-4681-ac6d-eda2851ee931",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = pd.DataFrame(adata.X, index=adata.obs_names.values, columns=adata.var.index)\n",
    "counts.index = [f\"{b}_{sample}\" for b in counts.index]\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920811e4-cb1b-403c-9533-9749492dc4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts.to_pickle(f\"{out_folder}/data/inputX/{sample}.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e24b78-6338-4f58-9fc6-5b76b6894c54",
   "metadata": {},
   "source": [
    "We load the pathology foundation model along with its preprocessing pipeline and feature dimensions. Currently, we support Phikon, Uni, DenseNet121, ResNet50, and Inception. However, this list can be extended to include more models by modifying the `get_morphology_model_and_preprocess` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d16a43-9c0d-4767-83fb-0b69b6bd34c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "morphology_model, preprocess, feature_dim = get_morphology_model_and_preprocess(model_name=image_feature_model, \n",
    "                                                                                device=device)\n",
    "feature_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f490e1c-7701-4765-8369-2c52447f9738",
   "metadata": {},
   "source": [
    "We now begin extracting tile representations and store them to save time during training by using the precomputed data. The representations are stored per spot, which includes the main representation and the k non-overlapping subspots, resulting in a shape of `(n_spots, k+1, feature_dim)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a987dba-2473-4fa2-99c2-175904e3eb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = pyvips.Image.new_from_file(img_path)\n",
    "morphology_model = morphology_model.to(device)\n",
    "barcode = adata.obs_names\n",
    "x_pixel = adata.obs.x_pixel\n",
    "y_pixel = adata.obs.y_pixel\n",
    "\n",
    "\n",
    "image = pyvips.Image.new_from_file(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a9f404-ed33-4217-b969-20a7ef4c1788",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_features = []\n",
    "\n",
    "batch_X = []\n",
    "batch_barcode = []\n",
    "batch_size = 128\n",
    "\n",
    "\n",
    "for i, (b, x, y) in tqdm(enumerate(zip(barcode, x_pixel, y_pixel))): \n",
    "    \n",
    "    patch = crop_tile(image, x, y, cell_diameter)\n",
    "    X = preprocess(patch)\n",
    "    \n",
    "    X = X.unsqueeze(0)\n",
    "    X = X.to(device)\n",
    "    X = X.float()\n",
    "\n",
    "    # Accumulate the preprocessed patches into a batch\n",
    "    batch_X.append(X)\n",
    "    batch_barcode.append(b)\n",
    "    if len(batch_X) == batch_size:\n",
    "        batch_tensor = torch.cat(batch_X, dim=0)  # Combine the list of tensors into one tensor\n",
    "        \n",
    "        \n",
    "        # We recommend using mixed precision for faster inference.\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.float32):\n",
    "            with torch.inference_mode():\n",
    "                output = morphology_model(batch_tensor)\n",
    "                output = output.detach().cpu().numpy()\n",
    "\n",
    "        for b, embedding in zip(batch_barcode, output):\n",
    "            np.save(f\"{folder_to_create}/{b}.npy\", embedding)\n",
    "        \n",
    "        batch_barcode.clear() # Reset the batch list\n",
    "        batch_X.clear()  # Reset the batch list\n",
    "\n",
    "# Process any remaining patches if they exist\n",
    "if batch_X and batch_barcode:\n",
    "    batch_tensor = torch.cat(batch_X, dim=0)\n",
    "    batch_X.clear()  # Reset the batch list\n",
    "    \n",
    "    # We recommend using mixed precision for faster inference.\n",
    "    with torch.autocast(device_type=\"cuda\", dtype=torch.float32):\n",
    "        with torch.inference_mode():\n",
    "            output = morphology_model(batch_tensor)\n",
    "            output = output.detach().cpu().numpy()\n",
    "                \n",
    "    for b, embedding in zip(batch_barcode, output):\n",
    "        np.save(f\"{folder_to_create}/{b}.npy\", embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912bfb50-d139-4876-8b18-c5b4e3d2a393",
   "metadata": {},
   "source": [
    "Each spot is encoded with its unique barcode per slide id and the slide id itslef."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d019245-7223-4443-bfae-275bd0f4e6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "glob.glob(f\"{out_folder}/data/image_features/{image_feature_model}_{cell_diameter}/{sample}/*\")[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c7f936-727b-4d56-aa67-8cbe1b3697e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepspot",
   "language": "python",
   "name": "deepspot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
