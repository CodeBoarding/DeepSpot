{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a95ec865-0b03-4e45-8f30-d1d219710b4f",
   "metadata": {},
   "source": [
    "## Spatial transcriptomics preprocessing\n",
    "\n",
    "In these GettingStarted notebooks, we will guide you through the process of preprocessing, training, and performing inference with DeepSpot on your spatial transcriptomics data. First, we will begin with data preprocessing. In the training notebook, we will demonstrate how to train DeepSpot, adjust hyperparameters, and export the model weights. Finally, in the inference notebook, we will show you how to load the model weights and perform spatial transcriptomics prediction using H&E images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27e85b9-37aa-4070-8537-a0c8d4f32f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../../')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7f9f4c-7bf3-44a8-8f4b-9c5f41a91546",
   "metadata": {},
   "source": [
    "Export packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212bc105-af0c-4214-bdff-944d46fceaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepspot.utils.utils_image import get_morphology_model_and_preprocess\n",
    "from deepspot.utils.utils_image import compute_mini_tiles\n",
    "from deepspot.utils.utils_image import crop_tile\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyvips\n",
    "import torch\n",
    "import glob\n",
    "import yaml\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef214d31-64ff-4f71-aee9-edf5e569beb9",
   "metadata": {},
   "source": [
    "Specify the input files. For this example, we have selected one sample from the COAD dataset [1], which was downloaded using the HEST1K pipeline [2]. You can modify this pipeline to process multiple samples. The goal is to help you understand the underlying logic.\n",
    "\n",
    "[1] Valdeolivas, A., Amberg, B., Giroud, N., Richardson, M., Gálvez, E. J., Badillo, S., ... & Hahn, K. (2023). Charting the heterogeneity of colorectal cancer consensus molecular subtypes using spatial transcriptomics. bioRxiv, 2023-01.\n",
    "\n",
    "[2] Jaume, G., Doucet, P., Song, A. H., Lu, M. Y., Almagro-Pérez, C., Wagner, S. J., ... & Mahmood, F. (2024). Hest-1k: A dataset for spatial transcriptomics and histology image analysis. arXiv preprint arXiv:2406.16192."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5102f8f2-2be4-43b6-892a-7db1cc97c312",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1308123d-004f-4fa5-b2f2-867d586affaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mini_tiles = 9 # number of non-overlaping sub-spots per subspot\n",
    "image_feature_model = \"inception\" # foundation model \n",
    "sample = \"ZEN38\" # COAD dataset sample\n",
    "out_folder = \"example_data\"\n",
    "adata_in = f\"example_data/data/h5ad/{sample}.h5ad\"\n",
    "json_path = f\"example_data/data/meta/{sample}.json\"\n",
    "img_path = f\"example_data/data/image/{sample}.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66cb206-2051-4430-93b0-6201780ea2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folder to save tile embeddings\n",
    "folder_to_create = f\"{out_folder}/data/image_features/{image_feature_model}/{sample}\"\n",
    "Path(folder_to_create).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "Path(f\"{out_folder}/data/inputX\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc433ca0-3699-4abb-9f0b-8e031b7a30aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_diameter_fullres = round(json.load(open(json_path))[\"spot_diameter_fullres\"]) # spot diameter\n",
    "spot_diameter_fullres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e229c5-149b-40ee-b191-ed72dc6dbf76",
   "metadata": {},
   "source": [
    "We start by loading the spatial transcriptomics data and selecting the most variable genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24572520-d00b-4e81-b692-8c743d7d1b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.read_h5ad(adata_in)\n",
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121cc4ce-a2a7-4406-9885-e2692b09c4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.highly_variable_genes(adata, flavor='seurat_v3_paper', \n",
    "                            n_top_genes=5000)\n",
    "adata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae3bfad-5c8a-414b-8eb8-486337783ad9",
   "metadata": {},
   "source": [
    "We select the most highly variable genes in the isPredicted variable and save this table. Later, it is useful to understand which genes are the most variable (e.g., based on their rank)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97177a69-27a7-4af3-b78d-278535dc590d",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.var[\"isPredicted\"] = adata.var.highly_variable.values\n",
    "adata.var[\"gene_name\"] = adata.var.index.values\n",
    "adata.var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef206c11-305f-47ac-84be-68400e4a3aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.var.to_csv(f\"{out_folder}/data/info_highly_variable_genes_Visium.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec431b0-b71b-4cad-afa5-a2e911282884",
   "metadata": {},
   "source": [
    "Here, we save only the counts and store them as a pickle file to make them quickly accessible during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6cc1c0-34b9-4681-ac6d-eda2851ee931",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = pd.DataFrame(adata.X, index=adata.obs_names.values, columns=adata.var.index)\n",
    "counts.index = [f\"{b}_{sample}\" for b in counts.index]\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920811e4-cb1b-403c-9533-9749492dc4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts.to_pickle(f\"{out_folder}/data/inputX/{sample}.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e24b78-6338-4f58-9fc6-5b76b6894c54",
   "metadata": {},
   "source": [
    "We load the pathology foundation model along with its preprocessing pipeline and feature dimensions. Currently, we support Phikon, Uni, DenseNet121, ResNet50, and Inception. However, this list can be extended to include more models by modifying the `get_morphology_model_and_preprocess` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d16a43-9c0d-4767-83fb-0b69b6bd34c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "morphology_model, preprocess, feature_dim = get_morphology_model_and_preprocess(model_name=image_feature_model, \n",
    "                                                                                device=device)\n",
    "feature_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f490e1c-7701-4765-8369-2c52447f9738",
   "metadata": {},
   "source": [
    "We now begin extracting tile representations and store them to save time during training by using the precomputed data. The representations are stored per spot, which includes the main representation and the k non-overlapping subspots, resulting in a shape of `(n_spots, k+1, feature_dim)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a987dba-2473-4fa2-99c2-175904e3eb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = pyvips.Image.new_from_file(img_path)\n",
    "morphology_model = morphology_model.to(device)\n",
    "barcode = adata.obs_names\n",
    "x_pixel = adata.obs.x_pixel\n",
    "y_pixel = adata.obs.y_pixel\n",
    "\n",
    "\n",
    "image = pyvips.Image.new_from_file(img_path)\n",
    "main_features = np.zeros([len(adata), feature_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9f0933-5ad2-43b7-8fc7-5bd41b3d1892",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5baa9b47-db20-4f13-b834-6fa26772ba48",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (b, x, y) in tqdm(enumerate(zip(barcode, x_pixel, y_pixel))): \n",
    "\n",
    "    main_tile = crop_tile(image, x, y, spot_diameter_fullres)\n",
    "    preprocess_main_tile = preprocess(main_tile)\n",
    "\n",
    "    X = np.zeros([n_mini_tiles + 1, 3, preprocess_main_tile.shape[1], preprocess_main_tile.shape[1]]) \n",
    "    X[0, :] = preprocess_main_tile\n",
    "\n",
    "    mini_tiles = compute_mini_tiles(main_tile, n_mini_tiles)\n",
    "    \n",
    "    for j, mini_tile in enumerate(mini_tiles):\n",
    "        \n",
    "        X[j+1, :] = preprocess(mini_tile)\n",
    "\n",
    "    \n",
    "    X = torch.from_numpy(X)\n",
    "    X = X.to(device).float()\n",
    "    # We recommend using mixed precision for faster inference.\n",
    "    with torch.autocast(device_type=\"cuda\", dtype=torch.float32):\n",
    "        with torch.inference_mode():\n",
    "            output = morphology_model(X)\n",
    "            output = output.float().detach().cpu().numpy()\n",
    "\n",
    "    main_features[i,:] = output[0]\n",
    "    \n",
    "    np.save(f\"{folder_to_create}/{b}.npy\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c7a3d8-2329-4336-93c1-fbc96a9eca31",
   "metadata": {},
   "outputs": [],
   "source": [
    "glob.glob(f\"{out_folder}/data/image_features/{image_feature_model}/{sample}/*\")[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912bfb50-d139-4876-8b18-c5b4e3d2a393",
   "metadata": {},
   "source": [
    "Each spot is encoded with its unique barcode per slide id and the slide id itslef."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08265e37-fb88-4791-8244-e27c36d868ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepspot",
   "language": "python",
   "name": "deepspot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
